---
title: "Pest Control"
author: "A Case Study in Bayesian Workflow"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 2
---

## Setup

```{r setup}
knitr::opts_chunk$set(
  echo = TRUE, 
  dev = "png",
  dpi = 150,
  fig.align = "center",
  comment = NA
)
library(rstan)
library(dplyr)
library(reshape2)
library(bayesplot)
library(ggplot2)

theme_set(bayesplot::theme_default())

# for R's pseudo-RNGs, not Stan's
set.seed(1123) 
```

## The problem

### Background

Let's say that you're a data scientist working as an independent contractor.
You're contacted by the property manager of a large property network in New York
City. They explain that they are concerned about the number of roach complaints
that they receive from their buildings. Previously they have offered monthly
visits from a pest inspector as a solution to this problem. While this is the
default solution of many property managers in NYC, the tenants are rarely home
when the inspector visits, and so the manager reasons that this is a relatively
expensive solution that is currently not very effective.

One alternative to this problem is to deploy long term bait stations. In this
alternative, child and pet safe bait stations are installed throughout the
apartment building. Cockroaches obtain quick acting poison from these stations
and distribute it throughout the colony. The manufacturer of these bait stations
provides some indication of the space-to-bait efficacy, but the manager suspects
that this guidance was not calculated with NYC roaches in mind. NYC roaches, the
manager rationalizes, have more hustle than traditional roaches; and NYC
buildings are built differently than other common residential buildings in the
US. This is particularly important as the unit cost for each bait station per
year is quite high.

### The goal

The manager wishes to employ your services to help them to find the optimal
number of roach bait stations they should place in each of their buildings in
order to minimize the number of cockroach complaints while also keeping
expenditure on pest control affordable. 

A subset of the company's buildings have been randomly selected for an experiment: 

* At the beginning of each month, a pest inspector randomly places a number of
bait stations throughout the building, without knowledge of the current
cockroach levels in the building
* At the end of the month, the manager records
the total number of cockroach complaints in that building. 
* The manager would like to determine the optimal number of traps ($\textrm{traps}$) that
balances the lost revenue ($R$) that complaints ($\textrm{complaints}$) generate
with the all-in cost of maintaining the traps ($\textrm{TC}$). 

Fortunately, Bayesian data analysis provides a coherent framework for us to tackle this problem.

Formally, we are interested in finding

$$
\arg\max_{\textrm{traps} \in \mathbb{N}} \mathbb{E}_{\text{complaints}}[R(\textrm{complaints}(\textrm{traps})) - \textrm{TC}(\textrm{traps})]
$$

The property manager would also, if possible, like to learn how these results 
generalize to buildings they haven't treated so they can understand the
potential costs of pest control at buildings they are acquiring as well as for
the rest of their building portfolio.

As the property manager has complete control over the number of traps set, the
random variable contributing to this expectation is the number of complaints
given the number of traps. We will model the number of complaints as a function
of the number of traps.

## The data

The data provided to us is in a file called `pest_data.RDS`. Let's
load the data and see what the structure is:

```{r load-data}
pest_data <- readRDS('data/pest_data.RDS')
str(pest_data)
```

We have access to the following fields: 

* `complaints`: Number of complaints per building per month
* `building_id`: The unique building identifier
* `traps`: The number of traps used per month per building
* `date`: The date at which the number of complaints are recorded
* `live_in_super`: An indicator for whether the building as a live-in super
* `age_of_building`: The age of the building
* `total_sq_foot`: The total square footage of the building
* `average_tenant_age`: The average age of the tenants per building
* `monthly_average_rent`: The average monthly rent per building
* `floors`: The number of floors per building

First, let's see how many buildings we have data for:

```{r describe-data}
N_buildings <- length(unique(pest_data$building_id))
N_buildings
```

And make some plots of the raw data: 

```{r data-plots}
ggplot(pest_data, aes(x = complaints)) + 
  geom_bar()

ggplot(pest_data, aes(x = traps, y = complaints)) + 
  geom_col()

ggplot(pest_data, aes(x = traps, y = complaints, color = live_in_super == TRUE)) + 
  geom_jitter()
```

```{r, data-plots-ts, fig.width = 6, fig.height = 8}
ggplot(pest_data, aes(x = date, y = complaints, color = live_in_super == TRUE)) + 
  geom_line(aes(linetype = "Number of complaints")) + 
  geom_point(color = "black") + 
  geom_line(aes(y = traps, linetype = "Number of traps"), color = "black", size = 0.25) + 
  facet_wrap(~building_id, scales = "free", ncol = 2, labeller = label_both) + 
  scale_x_date(name = "Month", date_labels = "%b") + 
  scale_y_continuous(name = "", limits = range(pest_data$complaints)) + 
  scale_linetype_discrete(name = "") + 
  scale_color_discrete(name = "Live-in super")
```

The first question we might want to ask is whether the number of complaints per 
building per month is a function of the number of bait stations per building per
month. That requires only two variables, $\textrm{complaints}$ and 
$\textrm{traps}$. How can we model the number of complaints? 


## Bayesian workflow

Insert steps here

## Modeling count data : Poisson distribution

We already know some rudimentary information about what we should expect. The
number of complaints over a month should be either zero or an integer. The
property manager tells us that it is possible but unlikely that number of
complaints in a given month is zero. Occasionally there are a very large number
of complaints in a single month. A common way of modelling this sort of skewed,
single bounded count data is as a Poisson random variable. One concern about
modelling the outcome variable as a Poisson is that the data may be
over-dispersed. How can we address this concern? We'll start with a simple
Poisson model and build it up slowly.

### Model 

Given that we have chosen a Poisson regression, we define the likelihood to be
the Poisson probability mass function over the number bait stations placed in
the building, denoted below as `traps`. This model assumes that the mean and
variance of the outcome variable `complaints` (number of complaints) is the
same. We'll investigate whether this is a good assumption after we fit the 
model.

For building $b = 1,\dots,10$ at time $t = 1,\dots,12$, we have

$$
\begin{align*}
\textrm{complaints}_{b,t} & \sim \textrm{Poisson}(\lambda_{b,t}) \\
\lambda_{b,t} & = \exp{(\eta_{b,t})} \\
\eta_{b,t} &= \alpha + \beta \, \textrm{traps}_{b,t}
\end{align*}
$$

Let's encode this probability model in Stan code. 

### Writing our first Stan model

### Making sure our code is right

However, before we fit the model, we need to walk through
the best way to go about developing models in Stan.


### Simulate some data

How do we know if our Stan model is working well and if we are able to recover
the known parameter values? Before we start fitting our Poisson model to the
real data, first let's generate some fake data that matches our assumptions
about the data.

First we will compile the Stan model (simple_poisson_regression_dgp.stan) that
generated the fake data.

```{r , cache=TRUE, results="hide", message=FALSE}
comp_dgp_simple <- stan_model('stan_programs/simple_poisson_regression_dgp.stan')
```

Next we use this model to sample some data. 

```{r runpoissondgp}
fitted_model_dgp <- sampling(
  comp_dgp_simple,
  data = list(N = nrow(pest_data), mean_traps = mean(pest_data$traps)),
  chains = 1,
  iter = 1,
  algorithm = 'Fixed_param',
  seed = 123
  )
samps_dgp <- rstan::extract(fitted_model_dgp)
str(samps_dgp)
```

In order to pass the fake data to our Stan program using RStan, we need to
arrange the data into a named list, whose elements correspond to the names
in our Stan program.

```{r}
stan_dat_fake <- list(
  N = nrow(pest_data), 
  traps = samps_dgp$traps[1, ], 
  complaints = samps_dgp$complaints[1, ]
)
str(stan_dat_fake)
```
### Fit the model to the fake data:

MOTIVATE INFORMATIVE PRIOR ON BETA

Now we have the simulated data we fit a Stan model using it. First we need to
compile the model (simple_poisson_regression.stan).

```{r , cache=TRUE, results="hide", message=FALSE}
comp_model_P <- stan_model('stan_programs/simple_poisson_regression.stan')
```

Lastly, let's run the model to see if we can recover our simulated parameters. 

```{r}
fit_model_P <- sampling(comp_model_P, data = stan_dat_fake)

# see http://mc-stan.org/rstan/articles/stanfit_objects.html for various
# ways of extracting the contents of the stanfit object
posterior_alpha_beta <- as.matrix(fit_model_P, pars = c('alpha','beta'))
head(posterior_alpha_beta)
```

### Assess parameter recovery

First explore if we can recover the data that we originally simulated from.

```{r}
true_alpha_beta <- c(samps_dgp$alpha, samps_dgp$beta)
mcmc_recover_hist(posterior_alpha_beta, true = true_alpha_beta)
```

It's possible we don't do a great job recovering the parameters here simply
because we're simulating so few observations that the posterior uncertainty
remains rather large. If we did the simulation with many more observations the
parameters would be estimated much more precisely.

Next we check some [Posterior Predictive Checks (PPCs)](http://mc-stan.org/bayesplot/articles/graphical-ppcs.html). 

One common check for regression models is
to plot the residuals against the observed values for $y$. While this plot is
what we would expect for a Poisson regression, it is relatively hard to
interpret.

```{r}
y_rep <- as.matrix(fit_model_P, pars = "y_rep")
mean_y_rep <- colMeans(y_rep)
std_resid <- (stan_dat_fake$complaints - mean_y_rep) / sqrt(mean_y_rep)
qplot(mean_y_rep, std_resid) + hline_at(2) + hline_at(-2)
```

One alternative to this is a rootogram. This is a plot of the expected counts
(continuous line) vs the observed counts (blue histogram). We can see the model
fits well because the observed histogram matched the expected counts relatively
well.

```{r}
ppc_rootogram(stan_dat_fake$complaints, yrep = y_rep)
```

### Fit with real data 

Now we have seen that we can sensibly recover the parameters from simulated data
so we have confidence that our model is correctly specified and that our Stan
program doesn't have any bugs in it. 

Next we can use this model with the real data that we observed and look at how 
the model fits the data. We'll again pass the data to RStan as a list:

```{r stan-data}
stan_dat_simple <- list(
  N = nrow(pest_data), 
  complaints = pest_data$complaints,
  traps = pest_data$traps
)
```

As we have already compiled the model, we can jump straight to sampling from it.

```{r fit_P_real_data, cache=TRUE}
fit_P_real_data <- sampling(comp_model_P, data = stan_dat_simple, chains = 4, cores =4 )
```

and printing the parameters. What do these tell us? 

```{r results_simple_P}
print(fit_P_real_data, pars = c('alpha','beta'))
```

We can also plot the posterior distributions: 

```{r hist_simple_P}
mcmc_hist(as.matrix(fit_P_real_data, pars = c('alpha','beta')))
```

As we expected, it appears the number of bait stations set in a building impacts
the number of complaints about cockroaches that were made in the following
month. However, we still need to consider how well the model fits.


### Posterior predictive checking

```{r}
y_rep <- as.matrix(fit_P_real_data, pars = "y_rep")
```

```{r marginal_PPC}
ppc_dens_overlay(y = stan_dat_simple$complaints, y_rep[1:200,])
```
This is a plot of the kernel density estimates of the observed data ($y$,
thicker curve) and 200 simulated data sets ($y_{rep}$, thin curves) from the
posterior predictive distribution. If the model was fitting the data well, there
would be little difference between the observed density and the simulated
density. However, as you can see here, the simulated density is not as dispersed
as the kernel density of the observed data, and doesn't seem to capture the rate
of zeros in the observed data. This indicates that the Poisson model may not be
the best model to use for this data.

Let's explore this further by looking directly at the proportion of zeros in the
real data and predicted data.
```{r}
prop_zero <- function(x) mean(x == 0)
ppc_stat(y = stan_dat_simple$complaints, yrep = y_rep, stat = "prop_zero")
```
The plot above shows the observed proportion of zeros (thick vertical line) and
a histogram of the proportion of zeros in each of the simulated data sets. It is
clear that the model does not capture this feature of the data well at all.

This next plot is a plot of the standardised residuals of the observed vs predicted number of complaints. 

```{r}
mean_y_rep <- colMeans(y_rep)
std_resid <- (stan_dat_simple$complaints - mean_y_rep) / sqrt(mean_y_rep)
qplot(mean_y_rep, std_resid) + hline_at(2) + hline_at(-2)
```

As you can see here, it looks as though we have more positive residuals than negative,
which indicates that the model tends to underestimate the number of complaints
that will be received.

The rootogram is another useful plot to compare the observed vs expected number
of complaints. This is a plot of the expected counts (continuous line) vs the
observed counts (blue histogram):

```{r}
ppc_rootogram(stan_dat_simple$complaints, yrep = y_rep)
```

If the model was fitting well these would be relatively similar, however in this
figure we can see the number of complaints is underestimated if there are few
complaints, over-estimated for medium numbers of complaints and underestimated
if there are a large number of complaints.

We can also view how the predicted number of complaints varies with the number
of traps. From this we can see that the model doesn't seem to fully capture the
data.

```{r}
ppc_intervals(
  y = stan_dat_simple$complaints, 
  yrep = y_rep,
  x = stan_dat_simple$traps
) + 
  labs(x = "Number of traps", y = "Number of complaints")
```

Specifically, the model doesn't capture the tails of the observed data very
well.
